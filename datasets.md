# Dataset_comparison

## 1. Udacity challenge
LINK: https://www.kaggle.com/kumaresanmanickavelu/lyft-udacity-challenge/discussion/101832

12 classes, 5000 images, possibly too much here

## 2. Ariel imagery

LINK: https://www.kaggle.com/humansintheloop/semantic-segmentation-of-aerial-imagery

6 classes, 72 images
Looks good for errors too
x4 with rotations?

## 3. Drone dataset
400 images
Questionable privacy
LINK: https://www.kaggle.com/bulentsiyah/semantic-drone-dataset

## 4. Tensorflow cityscape semantic segmentation

LINK: https://www.tensorflow.org/datasets/catalog/cityscapes
https://www.cityscapes-dataset.com/

30 classes

## 5. Multidigit MNIST

LINK https://www.kaggle.com/farhanhubble/multimnistm2nist?select=segmented.npy

5000 images, 11 classes

Good number, but not sure about labelling error potential for different classes